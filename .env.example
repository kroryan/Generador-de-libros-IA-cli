# ============================================================
# CONFIGURACIÓN DEL GENERADOR DE LIBROS IA
# Generador-de-libros-IA-cli - Fase 4 Optimizado
# ============================================================

# ==== CONFIGURACIÓN DE MODELOS LLM ====
# Tipo de modelo por defecto (ollama, openai, groq, deepseek, anthropic)
MODEL_TYPE=ollama

# ==== OLLAMA ====
OLLAMA_API_BASE=http://localhost:11434

# ==== OPENAI ====
# OPENAI_API_KEY=tu_clave_aquí
# OPENAI_API_BASE=https://api.openai.com/v1

# ==== GROQ ====
# GROQ_API_KEY=tu_clave_aquí
# GROQ_AVAILABLE_MODELS=qwen-qwq-32b,llama3-8b-8192,mixtral-8x7b-32768

# ==== DEEPSEEK ====
# DEEPSEEK_API_KEY=tu_clave_aquí
# DEEPSEEK_AVAILABLE_MODELS=deepseek-chat,deepseek-reasoner

# ==== ANTHROPIC ====
# ANTHROPIC_API_KEY=tu_clave_aquí
# ANTHROPIC_AVAILABLE_MODELS=claude-3-opus,claude-3-sonnet,claude-3-haiku

# ============================================================
# FASE 4: CONFIGURACIÓN CENTRALIZADA
# ============================================================

# ==== CONFIGURACIÓN DE REINTENTOS (RetryConfig) ====
# Número máximo de reintentos para operaciones fallidas
RETRY_MAX_ATTEMPTS=3

# Timeout general para operaciones (segundos)
RETRY_TIMEOUT=60

# Delay base entre reintentos (segundos)
RETRY_BASE_DELAY=1.0

# Delay máximo entre reintentos (segundos)
RETRY_MAX_DELAY=10.0

# Estrategia de backoff: exponential, linear, fixed
RETRY_BACKOFF_STRATEGY=exponential

# ==== CONFIGURACIÓN DE SOCKETIO (SocketIOConfig) ====
# Intervalo de ping para mantener conexión viva (segundos)
SOCKETIO_PING_INTERVAL=25

# Timeout de ping - CAMBIADO DE 72h A 1h (segundos)
# Valor anterior: 259200 (72 horas) - EXCESIVO
# Valor nuevo: 3600 (1 hora) - RAZONABLE
SOCKETIO_PING_TIMEOUT=3600

# Modo asíncrono: threading, eventlet, gevent
SOCKETIO_ASYNC_MODE=threading

# Orígenes CORS permitidos (* permite todos)
SOCKETIO_CORS_ORIGINS=*

# ==== CONFIGURACIÓN DE RATE LIMITING (RateLimitConfig) ====
# Delay por defecto entre llamadas a API (segundos)
RATE_LIMIT_DEFAULT_DELAY=0.5

# Delays específicos por proveedor (segundos)
# OpenAI: Límites más estrictos
RATE_LIMIT_OPENAI_DELAY=1.0

# Groq: Límites moderados
RATE_LIMIT_GROQ_DELAY=0.5

# DeepSeek: Límites estrictos
RATE_LIMIT_DEEPSEEK_DELAY=1.0

# Anthropic: Límites estrictos
RATE_LIMIT_ANTHROPIC_DELAY=1.0

# Ollama: Sin límites (local)
RATE_LIMIT_OLLAMA_DELAY=0.1

# ==== CONFIGURACIÓN DE CONTEXTO (ContextConfig) ====
# Tamaño de contexto limitado (para LLMs locales)
CONTEXT_LIMITED_SIZE=2000

# Tamaño de contexto estándar (para LLMs con ventana amplia)
CONTEXT_STANDARD_SIZE=8000

# Intervalo de savepoints (cada N capítulos)
CONTEXT_SAVEPOINT_INTERVAL=3

# Tamaño máximo de acumulación de contexto
CONTEXT_MAX_ACCUMULATION=5000

# Habilita el sistema de contexto inteligente
USE_INTELLIGENT_CONTEXT=true

# Configuración del tamaño de contexto
# "limited" = Optimizado para LLMs locales y modelos pequeños
# "standard" = Para modelos con contexto amplio (GPT-4, Claude, etc.)
MODEL_CONTEXT_SIZE=limited

# ==== CONFIGURACIÓN DE LLM (LLMConfig) ====
# Temperatura del modelo (0.0 = determinista, 1.0 = creativo)
LLM_TEMPERATURE=0.7

# Habilitar streaming de respuestas
LLM_STREAMING=true

# Top K sampling (número de tokens candidatos)
LLM_TOP_K=50

# Top P sampling (nucleus sampling)
LLM_TOP_P=0.9

# Penalización de repetición (1.0 = sin penalización)
LLM_REPEAT_PENALTY=1.1

# Máximo de tokens en respuestas (None = ilimitado)
# LLM_MAX_TOKENS=4096

# ==== CONFIGURACIÓN DE GENERACIÓN (GenerationConfig) ====
# Tema por defecto para generación de libros
GEN_DEFAULT_SUBJECT=Aventuras en un mundo cyberpunk

# Perfil del protagonista por defecto
GEN_DEFAULT_PROFILE=Protagonista rebelde en un entorno distópico

# Estilo narrativo por defecto
GEN_DEFAULT_STYLE=Narrativo-Épico-Imaginativo

# Género literario por defecto
GEN_DEFAULT_GENRE=Cyberpunk

# Formato de salida por defecto (docx, pdf)
GEN_DEFAULT_OUTPUT_FORMAT=docx

# Directorio de salida para libros generados
GEN_OUTPUT_DIRECTORY=./docs

# ==== CONFIGURACIÓN DE LOGS ====
# Nivel de logging (DEBUG, INFO, WARNING, ERROR)
LOG_LEVEL=INFO

# ==== CONFIGURACIÓN LEGACY (Compatible con Fase 1-3) ====
# Estas variables se mantienen para compatibilidad pero serán
# reemplazadas por las nuevas variables de Fase 4

# Configuración de timeouts para APIs (LEGACY - usar RETRY_TIMEOUT)
API_TIMEOUT=60

# Configuración de reintentos (LEGACY - usar RETRY_MAX_ATTEMPTS)
MAX_RETRIES=3

# Formato de salida por defecto (LEGACY - usar GEN_DEFAULT_OUTPUT_FORMAT)
DEFAULT_OUTPUT_FORMAT=docx

# Directorio de salida (LEGACY - usar GEN_OUTPUT_DIRECTORY)
OUTPUT_DIRECTORY=./docs

# ==== CONFIGURACIÓN DE STREAMING ====
# Tamaño máximo del buffer de palabras (caracteres)
# Modelos rápidos (GPT-4, Claude): 30
# Modelos locales (Ollama): 80
# Por defecto: 50
STREAMING_WORD_BUFFER_SIZE=50

# Delimitadores de palabras (separados sin espacios)
# Estos caracteres marcan el final de una palabra
STREAMING_WORD_DELIMITERS= ,.\n\t;:!?"'()[]{}

# ==== CONFIGURACIÓN DE FEW-SHOT LEARNING ====
# Activar/desactivar few-shot learning (true/false)
USE_FEW_SHOT_LEARNING=true

# Umbral de calidad para guardar ejemplos (0.0-1.0)
EXAMPLE_QUALITY_THRESHOLD=0.75

# Número máximo de ejemplos a incluir en prompts
MAX_EXAMPLES_PER_PROMPT=2

# Ruta de almacenamiento de ejemplos
EXAMPLES_STORAGE_PATH=./data/examples

# Auto-guardar ejemplos de alta calidad automáticamente
FEW_SHOT_AUTO_SAVE=true

# ==== CONFIGURACIÓN EXPERIMENTAL ====
# Habilitar features experimentales
ENABLE_EXPERIMENTAL_FEATURES=false

# Intervalo de micro-resúmenes (cada N secciones)
MICRO_SUMMARY_INTERVAL=3