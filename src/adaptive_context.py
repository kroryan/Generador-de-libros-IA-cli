from memory_manager import HierarchicalMemoryManager
from entity_extractor import EntityExtractor
from content_analyzer import ContentAnalyzer
from utils import print_progress, clean_think_tags, extract_content_from_llm_response
import re
import time

class AdaptiveContextSystem:
    """
    Sistema inteligente que gestiona el contexto para la generaci√≥n de libros
    de forma adaptativa, utilizando estrategias avanzadas para prevenir
    la sobrecarga de contexto y mantener la coherencia narrativa.
    """
    
    def __init__(self, title, framework, total_chapters, model_size="standard"):
        # Inicializar gestores de memoria y extractores
        self.memory_manager = HierarchicalMemoryManager(title, framework, total_chapters)
        self.entity_extractor = EntityExtractor()
        self.content_analyzer = ContentAnalyzer()
        
        # Informaci√≥n global del libro
        self.title = title
        self.framework = framework
        self.total_chapters = total_chapters
        self.current_chapter = 1
        
        # Seguimiento de estado
        self.generation_state = {
            "model_size": model_size,       # "small", "standard", "large"
            "context_mode": "standard",     # "minimal", "standard", "extended"
            "risk_level": "low",            # "low", "medium", "high", "critical"
            "strict_checking": False,       # Activar verificaci√≥n estricta
            "collapse_count": 0,            # Contador de colapsos detectados
            "recovery_attempts": 0,         # Intentos de recuperaci√≥n
        }
        
        # L√≠mites de tokens por modelo - L√çMITE UNIVERSAL para prevenir sobrecarga
        self.token_limits = {
            "small": 15000,       # Para modelos peque√±os (7B-9B)
            "standard": 30000,    # L√≠mite universal: 2K por debajo del m√°ximo de deepseek (32K)
            "large": 30000        # Mismo l√≠mite universal para todos
        }
        
        # Umbral de advertencia (porcentaje del l√≠mite)
        self.warning_threshold = 0.85  # Advertencia cuando se alcanza el 85% del l√≠mite
        
        # Umbral de activaci√≥n de compresi√≥n de emergencia (porcentaje del l√≠mite)
        self.emergency_threshold = 0.92  # Activar compresi√≥n de emergencia al 92% del l√≠mite
        
        # Estrategias avanzadas
        self.enable_progressive_generation = True  # Generar en trozos peque√±os
        self.enable_entity_tracking = True         # Seguimiento de personajes/lugares
        self.enable_narrative_planning = True      # Planificar antes de generar
        self.adaptive_polling_frequency = 5        # Verificar salud cada N secciones
        
        # Verificar si hay una configuraci√≥n de contexto en variables de entorno
        self._check_env_context_config()
        
        print_progress(f"üß† Sistema de contexto adaptativo inicializado: Modelo {model_size}")
    
    def _check_env_context_config(self):
        """Verifica si hay configuraci√≥n de contexto en variables de entorno"""
        import os
        
        # Detectar configuraci√≥n de tama√±o de modelo
        model_context_size = os.environ.get("MODEL_CONTEXT_SIZE", "").strip().lower()
        if model_context_size in ["limited", "small"]:
            self.set_model_size("small")
            print_progress("‚öôÔ∏è Configurando contexto limitado desde variable de entorno")
            
            # Para deepseek y modelos similares, usar l√≠mites m√°s estrictos
            model_name = os.environ.get("MODEL_NAME", "").lower()
            if "deepseek" in model_name:
                self.token_limits["small"] = 10000  # Reducir a√∫n m√°s para deepseek (era 12000)
                print_progress("‚ö†Ô∏è Ajustando l√≠mites para modelo deepseek")
                
                # Umbrales m√°s agresivos para deepseek
                self.warning_threshold = 0.75  # Advertencia al 75% (antes 85%)
                self.emergency_threshold = 0.85  # Compresi√≥n al 85% (antes 92%)
                print_progress("‚ö†Ô∏è Umbrales de compresi√≥n m√°s agresivos para deepseek")
                
                # Forzar contexto minimalista siempre para deepseek
                self.generation_state["context_mode"] = "minimal"
                self.generation_state["strict_checking"] = True
        elif model_context_size == "extended":
            self.set_model_size("large")
            print_progress("‚öôÔ∏è Configurando contexto extendido desde variable de entorno")
    
    def set_model_size(self, size):
        """Establece el tama√±o/capacidad del modelo en uso"""
        valid_sizes = ["small", "standard", "large"]
        if size in valid_sizes:
            self.generation_state["model_size"] = size
            # Ajustar autom√°ticamente modo de contexto seg√∫n tama√±o
            if size == "small":
                self.generation_state["context_mode"] = "minimal"
                self.generation_state["strict_checking"] = True
                print_progress("‚öôÔ∏è Modo de contexto m√≠nimo activado para modelo peque√±o")
            elif size == "standard":
                self.generation_state["context_mode"] = "standard"
                print_progress("‚öôÔ∏è Modo de contexto est√°ndar activado")
            else:
                self.generation_state["context_mode"] = "extended"
                print_progress("‚öôÔ∏è Modo de contexto extendido activado para modelo grande")
    
    def start_chapter(self, chapter_num, chapter_title, chapter_summary):
        """Inicializa la gesti√≥n de contexto para un nuevo cap√≠tulo"""
        self.current_chapter = chapter_num
        
        # Reiniciar memoria de corto y medio plazo
        self.memory_manager.clear_medium_term_memory()
        self.content_analyzer.reset_history()
        
        # Guardar resumen del cap√≠tulo en memoria a largo plazo
        self.memory_manager.update_long_term_memory(
            chapter_num=chapter_num,
            chapter_summary=chapter_summary
        )
        
        # Extraer entidades iniciales del resumen
        initial_entities = self.entity_extractor.extract_entities_from_text(
            chapter_summary,
            chapter_num=chapter_num,
            is_new_character=True  # Buscar agresivamente nuevos personajes
        )
        
        # Registrar informaci√≥n del cap√≠tulo
        chapter_info = {
            "title": chapter_title,
            "number": chapter_num,
            "summary": chapter_summary,
            "initial_entities": initial_entities
        }
        
        print_progress(f"üìë Iniciando cap√≠tulo {chapter_num}: {chapter_title}")
        print_progress(f"üîç Entidades detectadas: {len(initial_entities['characters'])} personajes, {len(initial_entities['locations'])} lugares")
        
        return chapter_info
    
    def get_context_for_section(self, section_position, idea, previous_content="", section_number=1, total_sections=1):
        """
        Obtiene el contexto √≥ptimo para generar una secci√≥n espec√≠fica,
        basado en la posici√≥n, contenido previo y modo de contexto actual.
        
        Args:
            section_position: Posici√≥n en cap√≠tulo ("inicio", "medio", "final")
            idea: Idea a desarrollar en esta secci√≥n
            previous_content: Contenido generado previamente en el cap√≠tulo
            section_number: N√∫mero de secci√≥n actual
            total_sections: Total de secciones en el cap√≠tulo
            
        Returns:
            dict: Contexto optimizado para esta secci√≥n
        """
        # Extraer entidades activas del contenido previo
        active_entities = []
        if previous_content:
            active_entities = self.entity_extractor.get_active_entities(
                previous_content, 
                chapter_num=self.current_chapter
            )
            
            # Actualizar memoria de corto plazo con contenido previo y entidades
            self.memory_manager.update_short_term_memory(
                new_text=previous_content,
                active_entities=active_entities
            )
        
        # Modo de contexto espec√≠fico para la etapa actual
        if self.generation_state["context_mode"] == "minimal":
            # Para modelos peque√±os o en riesgo alto: contexto ultra-minimalista
            context = self._get_minimal_context(section_position, idea, active_entities)
            
        elif self.generation_state["context_mode"] == "standard":
            # Modo est√°ndar: balance entre informaci√≥n y econom√≠a de tokens
            context = self._get_standard_context(section_position, idea, active_entities)
            
        else:  # extended
            # Modo extendido: m√°s informaci√≥n para modelos con mayor capacidad
            context = self._get_extended_context(section_position, idea, active_entities, previous_content)
        
        # Modificar seg√∫n posici√≥n en el cap√≠tulo
        context = self._adjust_for_section_position(
            context, 
            section_position, 
            section_number, 
            total_sections
        )
        
        # A√±adir idea a desarrollar
        context["current_idea"] = idea
        
        return context
    
    def _get_minimal_context(self, section_position, idea, active_entities=None):
        """
        Obtiene el contexto m√≠nimo para modelos peque√±os o situaciones de alto riesgo.
        Proporciona solo la informaci√≥n esencial para mantener coherencia.
        
        Args:
            section_position: Posici√≥n en el cap√≠tulo ("inicio", "medio", "final")
            idea: Idea a desarrollar
            active_entities: Lista de entidades activas en la escena actual
            
        Returns:
            dict: Contexto m√≠nimo optimizado
        """
        minimal_context = {}
        
        # 1. Esencia narrativa ultra-condensada del libro
        book_essence = self.memory_manager.get_book_essence(max_length=150)
        minimal_context["core_narrative"] = book_essence
        
        # 2. Personajes activos actuales (m√≠nimo imprescindible)
        if active_entities:
            minimal_context["active_entities"] = ", ".join(active_entities[:3])
        else:
            # Si no hay entidades activas, usar las m√°s importantes del cap√≠tulo
            key_entities = self.memory_manager.get_key_entities(
                chapter_num=self.current_chapter,
                max_entities=2
            )
            if key_entities:
                minimal_context["active_entities"] = ", ".join(key_entities)
        
        # 3. Gu√≠a simplificada seg√∫n posici√≥n
        if section_position == "inicio":
            minimal_context["position_guidance"] = "Esta es la apertura del cap√≠tulo."
        elif section_position == "final":
            minimal_context["position_guidance"] = "Esta es la conclusi√≥n del cap√≠tulo."
        else:
            minimal_context["position_guidance"] = "Continuando el desarrollo del cap√≠tulo."
        
        # Informaci√≥n absolutamente esencial del cap√≠tulo actual (una frase)
        chapter_summary = self.memory_manager.get_chapter_summary(
            self.current_chapter, 
            max_length=100
        )
        if chapter_summary:
            minimal_context["chapter_essence"] = chapter_summary
            
        return minimal_context
        
    def _get_standard_context(self, section_position, idea, active_entities=None):
        """
        Obtiene el contexto est√°ndar para la mayor√≠a de situaciones.
        Balance entre informaci√≥n y econom√≠a de tokens.
        
        Args:
            section_position: Posici√≥n en el cap√≠tulo ("inicio", "medio", "final")
            idea: Idea a desarrollar
            active_entities: Lista de entidades activas en la escena actual
            
        Returns:
            dict: Contexto est√°ndar optimizado
        """
        # Partir del contexto m√≠nimo y expandirlo
        context = self._get_minimal_context(section_position, idea, active_entities)
        
        # 1. Expandir esencia narrativa
        book_essence = self.memory_manager.get_book_essence(max_length=300)
        context["core_narrative"] = book_essence
        
        # 2. A√±adir contexto de √∫ltimos eventos relevantes
        recent_events = self.memory_manager.get_recent_events(max_events=3)
        if recent_events:
            context["recent_events"] = recent_events
            
        # 3. Expandir informaci√≥n sobre personajes activos
        if active_entities:
            # Obtener m√°s detalles sobre los personajes activos (hasta 3)
            character_details = {}
            for character in active_entities[:3]:
                # Obtener estado actual e informaci√≥n relevante del personaje
                char_info = self.memory_manager.get_character_state(character)
                if char_info:
                    character_details[character] = char_info
                    
            if character_details:
                context["character_states"] = character_details
                
        # 4. A√±adir contenido reciente si est√° en medio o final
        if section_position in ["medio", "final"]:
            recent_content = self.memory_manager.get_recent_content(max_paragraphs=2)
            if recent_content:
                context["recent_paragraphs"] = recent_content
                
        # 5. Gu√≠a para mantener tono y estilo
        context["style_guidance"] = self.memory_manager.get_writing_style()
        
        return context
        
    def _get_extended_context(self, section_position, idea, active_entities=None, previous_content=""):
        """
        Obtiene el contexto extendido para modelos grandes.
        Proporciona informaci√≥n detallada para m√°xima coherencia y calidad.
        
        Args:
            section_position: Posici√≥n en el cap√≠tulo ("inicio", "medio", "final")
            idea: Idea a desarrollar
            active_entities: Lista de entidades activas en la escena actual
            previous_content: Contenido generado previamente
            
        Returns:
            dict: Contexto extendido rico en detalles
        """
        # Partir del contexto est√°ndar y expandirlo
        context = self._get_standard_context(section_position, idea, active_entities)
        
        # 1. Narrativa completa del libro
        full_narrative = self.memory_manager.get_book_essence(max_length=500)
        context["core_narrative"] = full_narrative
        
        # 2. Historia completa de los cap√≠tulos anteriores
        previous_chapters = self.memory_manager.get_previous_chapters_summaries(
            current_chapter=self.current_chapter,
            max_chapters=3
        )
        if previous_chapters:
            context["previous_chapters"] = previous_chapters
            
        # 3. Detalles completos de personajes y lugares
        if active_entities:
            # Expandir informaci√≥n de todos los personajes activos
            character_details = {}
            for character in active_entities:
                # Obtener estado completo e historia del personaje
                char_info = self.memory_manager.get_character_state(character, include_history=True)
                if char_info:
                    character_details[character] = char_info
                    
            if character_details:
                context["character_states"] = character_details
        
        # 4. Reglas del mundo y sistema m√°gico-tecnol√≥gico
        world_rules = self.memory_manager.get_world_rules()
        if world_rules:
            context["world_rules"] = world_rules
            
        # 5. A√±adir m√°s contenido reciente si est√° en medio o final
        if section_position in ["medio", "final"]:
            recent_content = self.memory_manager.get_recent_content(max_paragraphs=5)
            if recent_content:
                context["recent_paragraphs"] = recent_content
                
        # 6. Arcos narrativos activos
        active_arcs = self.memory_manager.get_active_plot_arcs()
        if active_arcs:
            context["active_arcs"] = active_arcs
            
        # 7. Lista de eventos principales de todo el libro
        major_events = self.memory_manager.get_major_events()
        if major_events:
            context["major_events"] = major_events
            
        return context
        
    def _adjust_for_section_position(self, context, section_position, section_number, total_sections):
        """
        Ajusta el contexto seg√∫n la posici√≥n espec√≠fica dentro del cap√≠tulo.
        
        Args:
            context: Contexto base a ajustar
            section_position: Posici√≥n general en el cap√≠tulo ("inicio", "medio", "final")
            section_number: N√∫mero exacto de secci√≥n actual
            total_sections: Total de secciones en el cap√≠tulo
            
        Returns:
            dict: Contexto ajustado seg√∫n posici√≥n
        """
        adjusted_context = context.copy()
        
        # Calcular porcentaje de avance en el cap√≠tulo
        progress_percent = (section_number / total_sections) * 100
        
        # Preparar gu√≠a de posici√≥n precisa
        if section_position == "inicio":
            position_guidance = f"Estamos en el {progress_percent:.0f}% inicial del cap√≠tulo {self.current_chapter}. "
            if section_number == 1:
                position_guidance += "Esta es la primera secci√≥n del cap√≠tulo, establece la escena y presenta la situaci√≥n inicial."
            else:
                position_guidance += "Continuamos en la parte inicial del cap√≠tulo, desarrollando la premisa."
                
        elif section_position == "final":
            position_guidance = f"Estamos en el {progress_percent:.0f}% final del cap√≠tulo {self.current_chapter}. "
            if section_number == total_sections:
                position_guidance += "Esta es la √∫ltima secci√≥n del cap√≠tulo, concluye los eventos y prepara el siguiente cap√≠tulo."
            else:
                position_guidance += "Avanzamos hacia la conclusi√≥n del cap√≠tulo, intensificando la tensi√≥n narrativa."
                
        else:  # "medio"
            position_guidance = f"Estamos al {progress_percent:.0f}% del cap√≠tulo {self.current_chapter}. "
            position_guidance += "Desarrolla los eventos centrales del cap√≠tulo, mant√©n el ritmo narrativo."
            
        # Actualizar gu√≠a de posici√≥n
        adjusted_context["position_guidance"] = position_guidance
        
        # Ajustes espec√≠ficos por posici√≥n
        if section_position == "inicio":
            # Para inicio, priorizar presentaci√≥n y referencias a cap√≠tulos anteriores
            chapter_intro = self.memory_manager.get_chapter_introduction(self.current_chapter)
            if chapter_intro:
                adjusted_context["chapter_intro"] = chapter_intro
                
        elif section_position == "final":
            # Para final, priorizar cierre y conexi√≥n con siguiente cap√≠tulo
            next_chapter_hint = self.memory_manager.get_next_chapter_preview(self.current_chapter)
            if next_chapter_hint and self.current_chapter < self.total_chapters:
                adjusted_context["next_chapter_hint"] = next_chapter_hint
                
        return adjusted_context
    
    def create_chapter_summary(self, chapter_num, chapter_title, contents):
        """
        Crea un resumen condensado del cap√≠tulo a partir de su contenido.
        
        Args:
            chapter_num: N√∫mero del cap√≠tulo
            chapter_title: T√≠tulo del cap√≠tulo
            contents: Contenido completo del cap√≠tulo
            
        Returns:
            str: Resumen condensado del cap√≠tulo
        """
        # Si el contenido es muy corto, devolverlo como resumen
        if len(contents) < 200:
            return contents
            
        # Extraer p√°rrafos significativos (primero, algunos del medio, √∫ltimo)
        paragraphs = contents.split('\n\n')
        
        # Seleccionar p√°rrafos clave
        key_paragraphs = []
        
        # Incluir primer p√°rrafo si existe
        if paragraphs:
            key_paragraphs.append(paragraphs[0])
            
        # Incluir algunos p√°rrafos del medio si hay suficientes
        if len(paragraphs) > 4:
            mid_point = len(paragraphs) // 2
            key_paragraphs.append(paragraphs[mid_point])
            
        # Incluir √∫ltimo p√°rrafo si existe y es diferente del primero
        if len(paragraphs) > 1:
            key_paragraphs.append(paragraphs[-1])
            
        # Extraer entidades principales
        entities = self.entity_extractor.extract_entities_from_text(
            contents,
            chapter_num=chapter_num
        )
        
        # Construir resumen estructurado
        summary_parts = [
            f"Cap√≠tulo {chapter_num}: {chapter_title}",
            f"Personajes principales: {', '.join(entities['characters'][:5]) if entities['characters'] else 'No detectados'}",
            f"Ubicaciones: {', '.join(entities['locations'][:3]) if entities['locations'] else 'No detectadas'}",
            "\nResumen:",
            "\n".join(key_paragraphs)
        ]
        
        # Unir todo en un resumen cohesivo
        summary = "\n\n".join(summary_parts)
        
        # Almacenar este resumen en memoria
        self.memory_manager.update_long_term_memory(
            chapter_num=chapter_num,
            chapter_summary=summary
        )
        
        return summary
    
    def estimate_token_count(self, context):
        """
        Estima el n√∫mero de tokens en el contexto para controlar l√≠mites.
        Usa aproximaci√≥n basada en caracteres, que es m√°s r√°pida pero menos precisa.
        
        Args:
            context: Diccionario con elementos de contexto o cadena de texto
            
        Returns:
            int: N√∫mero estimado de tokens
        """
        # Para contexto en formato diccionario
        if isinstance(context, dict):
            total_chars = 0
            
            # Procesar cada clave del contexto
            for key, value in context.items():
                if isinstance(value, str):
                    total_chars += len(value)
                elif isinstance(value, list):
                    # Para listas, contar caracteres de cada elemento si son strings
                    for item in value:
                        if isinstance(item, str):
                            total_chars += len(item)
                elif isinstance(value, dict):
                    # Para diccionarios anidados, contar caracteres de valores
                    for v in value.values():
                        if isinstance(v, str):
                            total_chars += len(v)
            
            # Aproximaci√≥n general: en espa√±ol ~3.5 caracteres = 1 token
            estimated_tokens = total_chars / 3.5
            return int(estimated_tokens)
        
        # Para contexto en formato texto
        elif isinstance(context, str):
            # Aproximaci√≥n general: en espa√±ol ~3.5 caracteres = 1 token
            return int(len(context) / 3.5)
        
        # Tipo no compatible
        return 0
    
    def check_context_size(self, context, model_size="standard"):
        """
        Verifica si el contexto supera los l√≠mites establecidos y advierte/comprime
        si es necesario.
        
        Args:
            context: Diccionario con elementos de contexto
            model_size: Tama√±o del modelo para usar l√≠mites correspondientes
            
        Returns:
            tuple: (Contexto (posiblemente comprimido), dict con info de tama√±o)
        """
        # Estimar tokens en el contexto actual
        estimated_tokens = self.estimate_token_count(context)
        
        # Obtener l√≠mite para el tipo de modelo actual
        token_limit = self.token_limits.get(model_size, self.token_limits["standard"])
        
        # Calcular porcentaje de uso
        usage_percentage = (estimated_tokens / token_limit) * 100
        
        # Informaci√≥n de diagn√≥stico
        size_info = {
            "estimated_tokens": estimated_tokens,
            "limit": token_limit,
            "usage_percentage": round(usage_percentage, 1),
            "compressed": False,
            "emergency_compressed": False
        }
        
        # Si supera umbral de advertencia, mostrar alerta
        if usage_percentage > self.warning_threshold * 100:
            print_progress(f"‚ö†Ô∏è Advertencia: Uso alto de contexto ({usage_percentage:.1f}% del l√≠mite)")
        
        # Si supera umbral de emergencia, aplicar compresi√≥n agresiva
        if usage_percentage > self.emergency_threshold * 100:
            print_progress(f"üö® Contexto excesivo detectado: {estimated_tokens} tokens ({usage_percentage:.1f}%)")
            compressed_context = self.apply_emergency_compression(context)
            size_info["compressed"] = True
            size_info["emergency_compressed"] = True
            
            # Recalcular informaci√≥n
            new_tokens = self.estimate_token_count(compressed_context)
            size_info["estimated_tokens"] = new_tokens
            size_info["usage_percentage"] = round((new_tokens / token_limit) * 100, 1)
            size_info["tokens_saved"] = estimated_tokens - new_tokens
            
            print_progress(f"‚úÇÔ∏è Compresi√≥n de emergencia aplicada: {new_tokens} tokens ({size_info['usage_percentage']}%)")
            return compressed_context, size_info
            
        # Si est√° cercano al umbral, aplicar compresi√≥n moderada
        elif usage_percentage > self.warning_threshold * 100:
            compressed_context = self.apply_adaptive_compression(context)
            size_info["compressed"] = True
            
            # Recalcular informaci√≥n
            new_tokens = self.estimate_token_count(compressed_context)
            size_info["estimated_tokens"] = new_tokens
            size_info["usage_percentage"] = round((new_tokens / token_limit) * 100, 1)
            size_info["tokens_saved"] = estimated_tokens - new_tokens
            
            print_progress(f"‚úÇÔ∏è Compresi√≥n adaptativa aplicada: {new_tokens} tokens ({size_info['usage_percentage']}%)")
            return compressed_context, size_info
        
        # Contexto en rango seguro
        return context, size_info
    
    def apply_emergency_compression(self, context):
        """
        Aplica compresi√≥n de emergencia muy agresiva cuando estamos cerca del l√≠mite
        de tokens, priorizando solo el contenido absolutamente esencial.
        
        Args:
            context: Diccionario con elementos de contexto
            
        Returns:
            dict: Contexto ultra-comprimido
        """
        if not isinstance(context, dict):
            return context
            
        emergency_context = {}
        
        # Preservar solo elementos cr√≠ticos para continuidad
        if "recent_paragraphs" in context:
            # Recortar dr√°sticamente p√°rrafos recientes
            paragraphs = context["recent_paragraphs"].split('\n\n')
            if len(paragraphs) > 1:
                # Mantener solo el √∫ltimo p√°rrafo (el m√°s reciente)
                emergency_context["recent_paragraphs"] = paragraphs[-1]
            else:
                emergency_context["recent_paragraphs"] = context["recent_paragraphs"][:500]
        
        # Preservar personajes activos (cr√≠tico para consistencia)
        if "active_entities" in context:
            emergency_context["active_entities"] = context["active_entities"]
            
        # Mantener la idea actual sin modificar (cr√≠tico para la tarea)
        if "current_idea" in context:
            emergency_context["current_idea"] = context["current_idea"]
            
        # Gu√≠a de posici√≥n muy simplificada
        if "position_guidance" in context:
            parts = context["position_guidance"].split('.')
            if parts:
                emergency_context["position_guidance"] = parts[0] + "."
                
        # Para esencia narrativa, recortar brutalmente
        if "core_narrative" in context:
            core = context["core_narrative"]
            if len(core) > 200:
                # Extraer solo primera oraci√≥n o frase
                sentences = core.split('.')
                if sentences:
                    emergency_context["core_narrative"] = sentences[0].strip() + "."
            else:
                emergency_context["core_narrative"] = core
                
        print_progress("üö® Compresi√≥n de emergencia aplicada - Contexto cr√≠tico")
        return emergency_context
    
    def apply_adaptive_compression(self, context):
        """
        Aplica compresi√≥n adaptativa cuando estamos cerca del umbral de advertencia
        pero no en situaci√≥n cr√≠tica.
        
        Args:
            context: Diccionario con elementos de contexto
            
        Returns:
            dict: Contexto comprimido adaptativamente
        """
        if not isinstance(context, dict):
            return context
            
        compressed_context = context.copy()
        
        # Reducir longitud de p√°rrafos recientes
        if "recent_paragraphs" in compressed_context:
            paragraphs = compressed_context["recent_paragraphs"].split('\n\n')
            if len(paragraphs) > 3:
                # Mantener solo los 3 √∫ltimos p√°rrafos
                compressed_context["recent_paragraphs"] = '\n\n'.join(paragraphs[-3:])
            
        # Para elementos de narrativa, recortar solo si son extensos
        if "core_narrative" in compressed_context and len(compressed_context["core_narrative"]) > 500:
            # Reducir a la mitad aproximadamente
            sentences = compressed_context["core_narrative"].split('.')
            if len(sentences) > 5:
                # Tomar primera oraci√≥n y √∫ltimas 2-3
                reduced = sentences[0] + '. ' + '. '.join(sentences[-3:])
                compressed_context["core_narrative"] = reduced + '.'
                
        # Eliminar elementos menos cr√≠ticos si existen
        for key in ["world_rules", "semantic_context", "major_events"]:
            if key in compressed_context:
                del compressed_context[key]
                
        # Reducir informaci√≥n de personajes menos relevantes
        if "character_states" in compressed_context and isinstance(compressed_context["character_states"], dict):
            character_states = compressed_context["character_states"]
            # Mantener solo los 3 personajes m√°s mencionados
            if len(character_states) > 3:
                active_entities = compressed_context.get("active_entities", "").split(", ")
                # Priorizar los personajes activos en la escena actual
                priority_chars = [c for c in active_entities if c in character_states]
                
                # Si no hay suficientes, agregar hasta completar 3
                remaining = list(character_states.keys())
                for char in priority_chars:
                    if char in remaining:
                        remaining.remove(char)
                        
                # Combinar prioritarios con algunos restantes hasta tener 3
                final_chars = priority_chars + remaining[:max(0, 3-len(priority_chars))]
                
                # Crear diccionario reducido
                compressed_context["character_states"] = {
                    char: state for char, state in character_states.items() 
                    if char in final_chars
                }
        
        print_progress("‚úÇÔ∏è Compresi√≥n adaptativa aplicada - Optimizando contexto")
        return compressed_context
    
    def process_generated_content(self, content, section_position, idea):
        """
        Procesa y analiza el contenido generado para detectar problemas
        como colapsos del modelo, repeticiones excesivas o incoherencias.
        
        Args:
            content: Texto generado por el modelo
            section_position: Posici√≥n en el cap√≠tulo ("inicio", "medio", "final")
            idea: Idea que se intent√≥ desarrollar
            
        Returns:
            tuple: (Contenido procesado, an√°lisis, bool indicando si hay colapso)
        """
        if not content or len(content.strip()) < 20:
            print_progress("‚ö†Ô∏è Contenido generado demasiado corto o vac√≠o")
            return content, {"analysis": {"potential_issues": ["Contenido vac√≠o o muy corto"]}}, True
            
        # Utilizar ContentAnalyzer para detectar riesgo de colapso
        strict_mode = self.generation_state["strict_checking"]
        is_collapsed, detail, analysis = self.content_analyzer.detect_collapse_risk(content, strict_mode)
        
        if is_collapsed:
            # Incrementar contador de colapsos
            self.generation_state["collapse_count"] += 1
            print_progress(f"‚ö†Ô∏è Posible colapso detectado: {detail}")
            
            # Si hemos detectado varios colapsos seguidos, aumentar nivel de riesgo
            if self.generation_state["collapse_count"] > 2:
                self.generation_state["risk_level"] = "high"
                print_progress("üö® Nivel de riesgo aumentado a ALTO por colapsos repetidos")
                
                # Cambiar a modo de contexto m√≠nimo si el riesgo es alto
                if self.generation_state["context_mode"] != "minimal":
                    self.generation_state["context_mode"] = "minimal"
                    print_progress("üîÑ Cambiando a modo de contexto M√çNIMO para recuperaci√≥n")
        else:
            # Reiniciar contador de colapsos si el contenido es bueno
            if self.generation_state["collapse_count"] > 0:
                self.generation_state["collapse_count"] = 0
                
            # Si estamos en modo m√≠nimo pero el riesgo es bajo, considerar volver a modo est√°ndar
            if (self.generation_state["context_mode"] == "minimal" and 
                self.generation_state["risk_level"] == "high" and 
                len(self.content_analyzer.history) > 3):
                    
                self.generation_state["risk_level"] = "medium"
                print_progress("‚úÖ Reduciendo nivel de riesgo a MEDIO tras generaci√≥n estable")
        
        # Actualizar memoria con el contenido generado
        self.memory_manager.update_short_term_memory(new_text=content)
        
        # Extraer entidades del contenido para seguimiento
        entities = self.entity_extractor.extract_entities_from_text(
            content, 
            chapter_num=self.current_chapter
        )
        
        # Detectar y almacenar eventos importantes
        if len(content) > 200:  # Solo para contenido sustancial
            self.memory_manager.detect_and_store_events(content, entities)
        
        return content, {"analysis": analysis}, is_collapsed
    
    def regenerate_section_safely(self, llm, idea, section_position, chapter_info):
        """
        Intenta regenerar una secci√≥n de forma segura cuando se ha detectado
        un colapso o problema en la generaci√≥n anterior.
        
        Args:
            llm: Modelo de lenguaje a utilizar
            idea: Idea original a desarrollar
            section_position: Posici√≥n en el cap√≠tulo
            chapter_info: Informaci√≥n del cap√≠tulo actual
            
        Returns:
            str: Contenido regenerado
        """
        print_progress("üîÑ Regenerando secci√≥n con estrategia de recuperaci√≥n")
        
        # Guardar estado actual
        original_context_mode = self.generation_state["context_mode"]
        
        try:
            # Cambiar temporalmente a modo de contexto m√≠nimo
            self.generation_state["context_mode"] = "minimal"
            
            # Extraer solo lo esencial del cap√≠tulo
            chapter_title = chapter_info.get("title", "este cap√≠tulo")
            chapter_num = chapter_info.get("number", self.current_chapter)
            
            # Simplificar la idea al m√°ximo
            simple_idea = idea.split('.')[0] if '.' in idea else idea
            
            # Obtener un contexto ultra minimalista
            minimal_context = self._get_minimal_context(
                section_position,
                simple_idea,
                []  # Sin entidades activas para simplificar
            )
            
            # Crear un prompt muy directo y simple
            prompt = f"""
            Escribe un p√°rrafo narrativo para continuar esta historia.
            
            Estamos en el cap√≠tulo {chapter_num}: "{chapter_title}"
            
            Idea a desarrollar: {simple_idea}
            
            {minimal_context.get('core_narrative', '')}
            
            IMPORTANTE: Escribe SOLO texto narrativo en espa√±ol, sin encabezados ni metadata.
            """
            
            # Generar con temperatura ligeramente m√°s baja para estabilidad
            response = llm(prompt, temperature=0.7)
            
            # Procesar respuesta
            content = clean_think_tags(extract_content_from_llm_response(response))
            
            # Verificar si la regeneraci√≥n fue exitosa
            is_collapsed, _, _ = self.content_analyzer.detect_collapse_risk(content, False)
            
            if is_collapsed:
                # Si a√∫n falla, intentar una √∫ltima vez con prompt ultra simplificado
                emergency_prompt = f"""
                Escribe un solo p√°rrafo narrativo para el cap√≠tulo "{chapter_title}" 
                desarrollando esta idea: {simple_idea}.
                
                Texto narrativo directo, sin introducci√≥n ni comentarios meta:
                """
                
                response = llm(emergency_prompt, temperature=0.65)
                content = clean_think_tags(extract_content_from_llm_response(response))
                
                # Si a√∫n as√≠ falla, usar texto de contingencia
                is_still_collapsed, _, _ = self.content_analyzer.detect_collapse_risk(content, False)
                if is_still_collapsed or len(content.strip()) < 50:
                    return f"La historia sigui√≥ avanzando. Los personajes continuaron explorando {chapter_title}, enfrentando nuevos desaf√≠os mientras persegu√≠an su objetivo."
            
            print_progress("‚úÖ Regeneraci√≥n exitosa")
            return content
            
        except Exception as e:
            print_progress(f"Error en regeneraci√≥n: {str(e)}")
            return f"La narrativa continu√≥ desarroll√°ndose en {chapter_title}."
            
        finally:
            # Restaurar el modo de contexto original
            self.generation_state["context_mode"] = original_context_mode
    
    def health_check(self, llm):
        """
        Realiza un chequeo de salud del sistema para detectar
        posibles problemas antes de que ocurran.
        
        Args:
            llm: Modelo de lenguaje para probar si es necesario
            
        Returns:
            bool: True si el sistema est√° saludable
        """
        # Verificar estad√≠sticas de colapso
        collapse_stats = self.content_analyzer.get_collapse_stats()
        
        # Si hay demasiados colapsos recientes, el sistema no est√° saludable
        if collapse_stats["total_collapses"] > 3:
            self.generation_state["risk_level"] = "high"
            return False
            
        # Si el nivel de riesgo es alto, hacer una prueba simple
        if self.generation_state["risk_level"] in ["high", "critical"]:
            try:
                # Prueba simple para verificar si el modelo responde coherentemente
                test_prompt = "Completa esta frase de manera coherente: El sol brillaba en el cielo mientras los p√°jaros"
                response = llm(test_prompt, temperature=0.2)
                
                # Verificar si la respuesta parece coherente
                if len(response) < 10 or "error" in response.lower():
                    print_progress("‚ùå Prueba de salud fallida: respuesta incoherente")
                    return False
                    
            except Exception:
                print_progress("‚ùå Prueba de salud fallida: error en la generaci√≥n")
                return False
                
        # Sistema saludable por defecto
        return True

    def reset_and_rebuild_context(self, current_narrative_point, idea_to_continue=None, recent_content=None):
        """
        Reinicia el contexto y lo reconstruye con informaci√≥n esencial para continuar
        desde el punto actual de la narrativa manteniendo coherencia.
        
        Esta funci√≥n es crucial cuando se alcanza el l√≠mite de tokens y se necesita
        "refrescar" el contexto manteniendo continuidad narrativa.
        
        Args:
            current_narrative_point: Descripci√≥n del punto actual en la narrativa
            idea_to_continue: Idea espec√≠fica que se estaba desarrollando
            recent_content: Contenido m√°s reciente generado (√∫ltimos p√°rrafos)
            
        Returns:
            dict: Contexto reconstruido m√≠nimo pero suficiente para continuar
        """
        print_progress("üîÑ Reiniciando y reconstruyendo contexto para continuar generaci√≥n")
        
        # 1. Limpiar memoria de trabajo (pero preservar memoria a largo plazo)
        self.memory_manager.clear_medium_term_memory()
        
        # 2. Construir contexto minimalista pero suficiente
        rebuilt_context = {}
        
        # Identificar entidades activas en el contenido reciente
        active_entities = []
        if recent_content:
            # Extraer entidades clave del contenido reciente
            active_entities = self.entity_extractor.get_active_entities(
                recent_content,
                chapter_num=self.current_chapter,
                max_entities=3  # Limitar a las 3 m√°s relevantes
            )
            
            # A√±adir contenido reciente (√∫ltimos 1-2 p√°rrafos) como punto de continuaci√≥n
            paragraphs = recent_content.split('\n\n')
            if paragraphs:
                # Solo usar el √∫ltimo o los dos √∫ltimos p√°rrafos
                if len(paragraphs) > 1:
                    rebuilt_context["continuation_point"] = '\n\n'.join(paragraphs[-2:])
                else:
                    rebuilt_context["continuation_point"] = paragraphs[-1]
        
        # 3. A√±adir esencia narrativa ultra-condensada
        book_essence = self.memory_manager.get_book_essence(max_length=150)
        rebuilt_context["core_narrative"] = book_essence
        
        # 4. A√±adir descripci√≥n del punto actual si est√° disponible
        if current_narrative_point:
            rebuilt_context["current_point"] = current_narrative_point
            
        # 5. A√±adir idea a continuar desarrollando
        if idea_to_continue:
            rebuilt_context["current_idea"] = idea_to_continue
            
        # 6. A√±adir personajes activos con descripci√≥n m√≠nima
        if active_entities:
            character_mini_info = {}
            for character in active_entities[:3]:  # Solo los 3 m√°s importantes
                # Obtener informaci√≥n ultra-condensada del personaje
                mini_desc = self.memory_manager.get_character_state(
                    character, 
                    include_history=False, 
                    max_length=50  # Descripci√≥n muy breve
                )
                if mini_desc:
                    character_mini_info[character] = mini_desc
                    
            if character_mini_info:
                rebuilt_context["active_characters"] = character_mini_info
                
        # 7. A√±adir gu√≠a de posici√≥n actual
        rebuilt_context["position_guidance"] = f"Continuando el cap√≠tulo {self.current_chapter} desde donde qued√≥ la narraci√≥n."
        
        print_progress("‚úÖ Contexto reconstruido para continuar con la generaci√≥n")
        return rebuilt_context
    
    def manage_token_overflow(self, llm, content_so_far, idea_current, section_position, retry_count=0):
        """
        Maneja situaciones donde se ha excedido el l√≠mite de tokens del modelo,
        reiniciando el contexto y permitiendo continuar la generaci√≥n.
        
        Args:
            llm: Modelo de lenguaje a utilizar
            content_so_far: Contenido generado hasta el momento
            idea_current: Idea que se estaba desarrollando
            section_position: Posici√≥n en la secci√≥n ("inicio", "medio", "final")
            retry_count: N√∫mero de intentos realizados
            
        Returns:
            tuple: (Contenido generado, booleano indicando √©xito)
        """
        # Si ya hemos intentado demasiadas veces, abortar para evitar bucles
        if retry_count >= 3:
            print_progress("‚ö†Ô∏è Demasiados intentos de recuperaci√≥n, generando contenido de emergencia")
            return self._generate_emergency_continuation(content_so_far, idea_current), False
            
        # Crear punto actual narrativo m√°s preciso
        current_point = f"Estamos en el cap√≠tulo {self.current_chapter}, {section_position} de la secci√≥n, desarrollando: {idea_current[:50]}..."
        
        # Extraer √∫ltimos p√°rrafos como punto de continuaci√≥n
        recent_paragraphs = ""
        if content_so_far:
            paragraphs = content_so_far.split('\n\n')
            if len(paragraphs) > 2:
                recent_paragraphs = '\n\n'.join(paragraphs[-2:])  # √öltimos 2 p√°rrafos
            else:
                recent_paragraphs = content_so_far
        
        try:
            # 1. Reiniciar y reconstruir contexto
            fresh_context = self.reset_and_rebuild_context(
                current_point,
                idea_current,
                recent_paragraphs
            )
            
            # 2. Preparar prompt para continuaci√≥n directa
            continuation_prompt = f"""
            Contin√∫a exactamente desde este punto de la narraci√≥n, 
            manteniendo el mismo tono y estilo.
            
            CONTEXTO ACTUAL:
            {fresh_context.get('core_narrative', '')}
            
            PUNTO ACTUAL:
            {fresh_context.get('current_point', '')}
            
            IDEA A DESARROLLAR:
            {fresh_context.get('current_idea', '')}
            
            √öLTIMO CONTENIDO GENERADO:
            {fresh_context.get('continuation_point', '')}
            
            IMPORTANTE: Contin√∫a la narraci√≥n de forma natural, como si no hubiera
            habido interrupci√≥n. No repitas lo anterior ni hagas res√∫menes.
            Escribe SOLO texto narrativo en espa√±ol, sin encabezados ni metadata.
            """
            
            # 3. Generar continuaci√≥n con temperatura ligeramente reducida para estabilidad
            response = llm(continuation_prompt, temperature=0.7)
            
            # 4. Procesar respuesta
            from utils import clean_think_tags, extract_content_from_llm_response
            new_content = clean_think_tags(extract_content_from_llm_response(response))
            
            # 5. Verificar calidad
            is_collapsed, detail, _ = self.content_analyzer.detect_collapse_risk(new_content, False)
            
            if is_collapsed or len(new_content.strip()) < 50:
                # Si falla, intentar una vez m√°s con un prompt a√∫n m√°s simplificado
                print_progress(f"‚ö†Ô∏è Continuaci√≥n problem√°tica: {detail}. Reintentando con prompt simplificado.")
                return self.manage_token_overflow(llm, content_so_far, idea_current, section_position, retry_count + 1)
            
            # 6. Si tenemos buen contenido, combinar con el contenido anterior
            if content_so_far and not content_so_far.endswith((".", "!", "?", ":", ";", "‚Äî")):
                # Si el contenido anterior no termina con puntuaci√≥n, a√±adir punto
                combined_content = content_so_far + ". " + new_content
            else:
                # Si el contenido anterior tiene puntuaci√≥n final, simplemente a√±adir nueva l√≠nea
                combined_content = content_so_far + "\n\n" + new_content
                
            print_progress("‚úÖ Continuaci√≥n generada exitosamente tras reinicio de contexto")
            return combined_content, True
            
        except Exception as e:
            print_progress(f"Error durante manejo de overflow: {str(e)}")
            return self._generate_emergency_continuation(content_so_far, idea_current), False
    
    def _generate_emergency_continuation(self, content_so_far, idea_current):
        """
        Genera un contenido de emergencia cuando todos los intentos de
        recuperaci√≥n han fallado.
        
        Args:
            content_so_far: Contenido generado hasta ahora
            idea_current: Idea que se estaba desarrollando
            
        Returns:
            str: Contenido de emergencia para continuar
        """
        # Extraer palabras clave de la idea actual
        keywords = idea_current.split()[:5]  # Primeras 5 palabras
        keywords_text = " ".join(keywords)
        
        # Generar texto gen√©rico que podr√≠a funcionar en cualquier situaci√≥n
        emergency_text = f"""
        
        La narrativa avanz√≥ mientras los personajes enfrentaban nuevos desaf√≠os. 
        {keywords_text} se convirti√≥ en el centro de atenci√≥n mientras 
        la historia continuaba desarroll√°ndose con giros inesperados.
        
        """
        
        if content_so_far:
            combined = content_so_far + "\n\n" + emergency_text
            return combined
        else:
            return emergency_text