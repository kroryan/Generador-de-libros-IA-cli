from utils import BaseEventChain, print_progress, clean_think_tags, extract_content_from_llm_response
from chapter_summary import ChapterSummaryChain, ProgressiveContextManager
from time import sleep
import re
import time  # Importaci√≥n a√±adida para usar time.sleep()

class WriterChain(BaseEventChain):
    PROMPT_TEMPLATE = """
    Eres un escritor profesional de {genre} en espa√±ol.
    
    ### INFORMACI√ìN ESENCIAL:
    - T√≠tulo: "{title}"
    - Estilo: {style}
    - Cap√≠tulo actual: {current_chapter} de {total_chapters}
    - Posici√≥n: {section_position} del cap√≠tulo
    
    ### MARCO NARRATIVO RESUMIDO:
    {framework_summary}
    
    ### CONTEXTO ANTERIOR COMPACTO:
    {minimal_context}
    
    ### CONTEXTO INMEDIATO:
    - T√≠tulo del cap√≠tulo: "{chapter_title}"
    - Secci√≥n: {section_number} de {total_sections}
    
    ### P√ÅRRAFOS RECIENTES:
    {previous_paragraphs}
    
    ### IDEA A DESARROLLAR AHORA:
    {current_idea}
    
    <think>
    Desarrollar√© esta idea enfoc√°ndome solo en:
    1. Conexi√≥n directa con el contenido reciente
    2. Desarrollo coherente de personajes y situaciones
    3. Avance natural de la historia
    
    Mantendr√© el foco narrativo sin divagar ni resumir.
    </think>
    
    IMPORTANTE: 
    - Escribe EXCLUSIVAMENTE texto narrativo en espa√±ol
    - NO incluyas notas, comentarios ni explicaciones
    - NO hagas res√∫menes ni menciones de la estructura
    - Solo genera el texto que formar√≠a parte del libro final
    
    Escribe directamente el contenido narrativo:"""

    def run(
        self,
        genre,
        style,
        title,
        context_manager,
        chapter_title,
        summary,
        previous_paragraphs,
        current_idea,
        current_chapter,
        total_chapters,
        section_position,
        section_number,
        total_sections,
        chapter_key
    ):
        print_progress(f"Escribiendo secci√≥n {section_number}/{total_sections} del cap√≠tulo {current_chapter}")
        
        try:
            # Obtener contexto m√≠nimo optimizado
            context_data = context_manager.get_context_for_section(current_chapter, section_position, chapter_key)
            
            # Crear un contexto mucho m√°s compacto
            minimal_context = self._create_minimal_context(
                context_data, 
                current_chapter, 
                section_position
            )
            
            # Extraer solo lo esencial del marco narrativo
            framework_summary = self._extract_framework_essence(context_data["framework"])
            
            # Limpiar y reducir las entradas para minimizar tokens
            previous_paragraphs_cleaned = self._optimize_previous_paragraphs(previous_paragraphs)
            chapter_summary_short = self._optimize_summary(summary)
            idea_to_develop = self._optimize_idea(current_idea)
            
            # T√≠tulo del cap√≠tulo simplificado (solo el nombre principal)
            simple_chapter_title = self._simplify_chapter_title(chapter_title)
            
            result = self.invoke(
                genre=clean_think_tags(genre),
                style=clean_think_tags(style),
                title=clean_think_tags(title),
                framework_summary=framework_summary,
                minimal_context=minimal_context,
                chapter_title=simple_chapter_title,
                previous_paragraphs=previous_paragraphs_cleaned,
                current_idea=clean_think_tags(idea_to_develop),
                current_chapter=current_chapter,
                total_chapters=total_chapters,
                section_position=section_position,
                section_number=section_number,
                total_sections=total_sections
            )
            
            # Eliminar posibles metadatos que el modelo haya incluido
            clean_result = self._clean_result(result)
            
            print_progress(f"Secci√≥n completada: {len(clean_result)} caracteres")
            return clean_result

        except Exception as e:
            print_progress(f"Error generando contenido: {str(e)}")
            raise
    
    def _create_minimal_context(self, context_data, current_chapter, position):
        """Crea un contexto m√≠nimo y √≥ptimo basado en la posici√≥n en el cap√≠tulo"""
        context_parts = []
        
        # Para inicio de cap√≠tulo: contexto de cap√≠tulos anteriores
        if position == "inicio" and current_chapter > 1:
            # Reducir dr√°sticamente el contexto previo
            prev_chapters = context_data.get("previous_chapters_summary", "")
            if prev_chapters:
                context_parts.append(f"Cap√≠tulos anteriores: {prev_chapters[:300]}...")
        
        # Para medio y final: resumen incremental del mismo cap√≠tulo
        if position in ["medio", "final"]:
            current_context = context_data.get("current_chapter_summary", "")
            if current_context:
                context_parts.append(f"Desarrollo previo: {current_context[:250]}...")
        
        return "\n".join(context_parts)
    
    def _extract_framework_essence(self, framework):
        """Extrae solo los elementos esenciales del marco narrativo"""
        if not framework:
            return ""
        
        # Limitar a 150 caracteres
        if len(framework) > 150:
            return framework[:150] + "..."
        return framework
    
    def _optimize_previous_paragraphs(self, text):
        """Optimiza los p√°rrafos previos para mantener solo lo m√°s reciente y relevante"""
        if not text:
            return ""
            
        # Limpiar etiquetas y metadata
        cleaned = clean_think_tags(text)
        
        # Reducir dr√°sticamente - solo mantener los √∫ltimos 2-3 p√°rrafos
        paragraphs = cleaned.split('\n\n')
        if len(paragraphs) > 3:
            return "...\n\n" + "\n\n".join(paragraphs[-3:])
        return cleaned
    
    def _optimize_summary(self, summary):
        """Reduce el resumen a lo esencial"""
        if not summary:
            return ""
            
        cleaned = clean_think_tags(summary)
        if len(cleaned) > 200:
            return cleaned[:200] + "..."
        return cleaned
    
    def _optimize_idea(self, idea):
        """Optimiza la idea a desarrollar"""
        if not idea:
            return ""
            
        cleaned = clean_think_tags(idea)
        if len(cleaned) > 150:
            return cleaned[:150] + "..."
        return cleaned
    
    def _simplify_chapter_title(self, title):
        """Simplifica el t√≠tulo del cap√≠tulo a su esencia"""
        if not title:
            return ""
            
        # Eliminar descripciones extensas despu√©s de dos puntos
        if ':' in title:
            return title.split(':', 1)[0].strip()
        return title
    
    def _clean_result(self, text):
        """Elimina posibles metadatos del resultado generado"""
        if not text:
            return ""
            
        # Eliminar patrones comunes que no pertenecen a la narrativa
        import re
        patterns = [
            r'^Cap√≠tulo \d+:.*?\n',
            r'^CAP√çTULO \d+:.*?\n',
            r'^Secci√≥n \d+:.*?\n',
            r'^\[Contin√∫a.*?\].*?\n',
            r'^\[Desarrollo.*?\].*?\n',
            r'^\[.*?\].*?\n',
            r'^\*\*\*.*?\*\*\*\n',
            r'^---.*?---\n'
        ]
        
        result = text
        for pattern in patterns:
            result = re.sub(pattern, '', result, flags=re.MULTILINE)
            
        return result.strip()

def regenerate_problematic_section(writer_chain, context_manager, section_params, max_attempts=3):
    """
    Intenta regenerar una secci√≥n que ha mostrado problemas o se√±ales de colapso.
    Usa estrategias progresivamente m√°s agresivas para recuperar la generaci√≥n.
    
    Args:
        writer_chain: Instancia de WriterChain para generar contenido
        context_manager: Gestor de contexto progresivo
        section_params: Par√°metros de la secci√≥n a regenerar
        max_attempts: N√∫mero m√°ximo de intentos antes de usar fallback
        
    Returns:
        str: Contenido regenerado
    """
    from utils import print_progress
    
    print_progress("üîÑ Intentando regenerar secci√≥n problem√°tica...")
    
    # Extractores de par√°metros clave
    chapter_key = section_params.get('chapter_key', 'cap√≠tulo actual')
    chapter_title = section_params.get('chapter_title', 'este cap√≠tulo')
    current_chapter = section_params.get('current_chapter', 1)
    section_position = section_params.get('section_position', 'medio')
    
    # Guardar capacidad original del modelo
    original_capacity = context_manager.model_capacity
    
    for attempt in range(max_attempts):
        try:
            print_progress(f"Intento {attempt+1}/{max_attempts} de regeneraci√≥n")
            
            # Estrategias progresivamente m√°s agresivas
            if attempt == 0:
                # Primer intento: reducir ligeramente el contexto
                if original_capacity != "limited":
                    context_manager.set_model_capacity("limited")
            elif attempt == 1:
                # Segundo intento: aplicar compresi√≥n de emergencia
                section_params['previous_paragraphs'] = section_params.get('previous_paragraphs', '')[:200]
                section_params['current_idea'] = section_params.get('current_idea', '')[:100]
            else:
                # √öltimo intento: contexto ultra minimalista
                minimal_context = context_manager.get_minimal_context()
                # Usar solo la primera l√≠nea de la idea actual
                current_idea = section_params.get('current_idea', '')
                idea_first_line = current_idea.split('\n')[0] if current_idea else ''
                
                # Crear un prompt con instrucciones absolutamente claras
                from utils import clean_think_tags, extract_content_from_llm_response
                
                emergency_prompt = f'''
                Escribe un p√°rrafo narrativo para continuar esta historia.
                
                Cap√≠tulo: {chapter_title}
                Contexto m√≠nimo: {minimal_context.get("framework", "")}
                Idea a desarrollar: {idea_first_line}
                
                IMPORTANTE: Escribe SOLO texto narrativo en espa√±ol, sin encabezados ni metadata.
                '''
                
                response = writer_chain.llm.invoke(emergency_prompt)
                return clean_think_tags(extract_content_from_llm_response(response))
            
            # Intentar la generaci√≥n con los par√°metros actuales
            content = writer_chain.run(**section_params)
            
            # Verificar si la regeneraci√≥n fue exitosa
            is_collapsed, message = context_manager.detect_collapse_risk(content)
            if not is_collapsed:
                # Restaurar capacidad del modelo
                context_manager.set_model_capacity(original_capacity)
                print_progress("‚úÖ Regeneraci√≥n exitosa")
                return content
                
        except Exception as e:
            print_progress(f"Error en intento {attempt+1}: {str(e)}")
    
    # Si todos los intentos fallan, usar un texto de contingencia
    context_manager.set_model_capacity(original_capacity)
    print_progress("‚ö†Ô∏è Usando texto de contingencia tras m√∫ltiples fallos")
    
    return f"La escena continu√≥ desarroll√°ndose. Los personajes avanzaron en su objetivo mientras exploraban {chapter_title}."

def write_book(genre, style, profile, title, framework, summaries_dict, idea_dict, chapter_summaries=None):
    print_progress("Iniciando escritura del libro...")
    writer_chain = WriterChain()
    summary_chain = ChapterSummaryChain()
    book = {}
    
    # Si no hay res√∫menes de cap√≠tulos, crear un diccionario vac√≠o
    if chapter_summaries is None:
        chapter_summaries = {}
        
    # Inicializar el nuevo sistema de contexto adaptativo
    from adaptive_context import AdaptiveContextSystem
    from utils import detect_model_size
    from content_analyzer import ContentAnalyzer
    
    # Detectar autom√°ticamente el tama√±o del modelo
    model_size = detect_model_size(writer_chain.llm)
    
    # Configurar el sistema adaptativo de contexto
    context_system = AdaptiveContextSystem(title, framework, len(idea_dict), model_size)
    content_analyzer = ContentAnalyzer()  # Analizador de calidad y colapsos
    
    # Simplificar el perfil para reducir contexto
    profile_short = profile[:150] + "..." if len(profile) > 150 else profile
    
    # Contador para chequeos peri√≥dicos de salud
    health_check_counter = 0 
    health_check_interval = 5  # Cada cu√°ntas secciones hacer un chequeo

    try:
        total_chapters = len(idea_dict)
        for i, (chapter, idea_list) in enumerate(idea_dict.items(), 1):
            print_progress(f"======================================")
            print_progress(f"CAP√çTULO {i}/{total_chapters}: {chapter}")
            print_progress(f"======================================")
            
            book[chapter] = []
            ideas_total = len(idea_list)
            chapter_content = []
            
            # Simplificar el t√≠tulo del cap√≠tulo
            chapter_title = chapter.split(':', 1)[0] if ':' in chapter else chapter
            
            # Obtener o crear resumen del cap√≠tulo
            chapter_summary = summaries_dict.get(chapter, "")
            
            # Inicializar contexto para este cap√≠tulo usando el nuevo sistema adaptativo
            chapter_info = context_system.start_chapter(
                chapter_num=i,
                chapter_title=chapter_title,
                chapter_summary=chapter_summary
            )

            for j, idea in enumerate(idea_list, 1):
                # Realizar chequeo peri√≥dico de salud del modelo
                health_check_counter += 1
                if health_check_counter >= health_check_interval:
                    health_check_counter = 0
                    is_healthy = context_system.health_check(writer_chain.llm)
                    if not is_healthy:
                        print_progress("‚è≥ Pausando brevemente para recuperar estabilidad...")
                        # Una pausa breve puede ayudar a "resetear" el contexto interno del modelo
                        time.sleep(2)
                
                try:
                    # Determinar posici√≥n en el cap√≠tulo
                    if j == 1:
                        section_position = "inicio"
                    elif j == ideas_total:
                        section_position = "final"
                    else:
                        section_position = "medio"
                    
                    # Mostrar solo parte de la idea para no llenar la consola
                    idea_preview = idea[:40] + "..." if len(idea) > 40 else idea
                    print_progress(f">> Idea {j}/{ideas_total}: {idea_preview}")
                    
                    # Obtener contexto adaptativo para esta secci√≥n espec√≠fica
                    chapter_paragraphs = "\n\n".join(chapter_content[-3:]) if chapter_content else ""
                    
                    adaptive_context = context_system.get_context_for_section(
                        section_position=section_position,
                        idea=idea,
                        previous_content=chapter_paragraphs,
                        section_number=j,
                        total_sections=ideas_total
                    )
                    
                    # NUEVO: Verificar y ajustar el tama√±o del contexto para no exceder el l√≠mite universal
                    adaptive_context, size_info = context_system.check_context_size(
                        adaptive_context, 
                        context_system.generation_state["model_size"]
                    )
                    
                    # Mostrar informaci√≥n del contexto si fue comprimido
                    if size_info["compressed"]:
                        print_progress(f"üìä Contexto: {size_info['estimated_tokens']} tokens ({size_info['usage_percentage']}% del l√≠mite)")
                        if size_info["emergency_compressed"]:
                            print_progress("‚ö†Ô∏è Se aplic√≥ compresi√≥n de emergencia para evitar exceder el l√≠mite")
                    
                    # Generar contenido usando WriterChain con el contexto adaptativo
                    prompt = f"""
                    Eres un escritor profesional de {genre} en espa√±ol.
                    
                    ### INFORMACI√ìN ESENCIAL:
                    - T√≠tulo: "{title}"
                    - Estilo: {style}
                    - Cap√≠tulo actual: {i} de {total_chapters}
                    - Posici√≥n: {section_position} del cap√≠tulo
                    
                    ### MARCO NARRATIVO:
                    {adaptive_context.get('core_narrative', '')}
                    
                    ### CONTEXTO RECIENTE:
                    {adaptive_context.get('recent_paragraphs', '')}
                    
                    ### PERSONAJES ACTIVOS:
                    {adaptive_context.get('active_entities', '')}
                    
                    ### GU√çA DE POSICI√ìN:
                    {adaptive_context.get('position_guidance', '')}
                    
                    ### IDEA A DESARROLLAR AHORA:
                    {adaptive_context.get('current_idea', '')}
                    
                    <think>
                    Desarrollar√© esta idea enfoc√°ndome solo en:
                    1. Conexi√≥n directa con el contenido reciente
                    2. Desarrollo coherente de personajes y situaciones
                    3. Avance natural de la historia
                    
                    Mantendr√© el foco narrativo sin divagar ni resumir.
                    </think>
                    
                    IMPORTANTE: 
                    - Escribe EXCLUSIVAMENTE texto narrativo en espa√±ol
                    - NO incluyas notas, comentarios ni explicaciones
                    - NO hagas res√∫menes ni menciones de la estructura
                    - Solo genera el texto que formar√≠a parte del libro final
                    
                    Escribe directamente el contenido narrativo:
                    """
                    
                    # Generar contenido de secci√≥n
                    try:
                        response = writer_chain.llm(prompt)
                        section_content = clean_think_tags(extract_content_from_llm_response(response))
                    except Exception as e:
                        # Capturar espec√≠ficamente errores de exceso de contexto/tokens
                        error_str = str(e).lower()
                        if any(keyword in error_str for keyword in ["token", "context length", "maximum context", "too many tokens"]):
                            print_progress("üö® Error de l√≠mite de tokens detectado, reiniciando contexto...")
                            
                            # Usar el nuevo sistema de manejo de overflow de tokens
                            if chapter_content:
                                # Si ya tenemos contenido previo, intentar continuar desde all√≠
                                recovered_content, success = context_system.manage_token_overflow(
                                    llm=writer_chain.llm,
                                    content_so_far="\n\n".join(chapter_content),
                                    idea_current=idea,
                                    section_position=section_position
                                )
                                
                                if success:
                                    # Extraer solo la parte nueva (lo que se acaba de generar)
                                    existing_content = "\n\n".join(chapter_content)
                                    if recovered_content.startswith(existing_content):
                                        # Si la recuperaci√≥n incluye todo el contenido anterior,
                                        # extraer solo la parte nueva
                                        section_content = recovered_content[len(existing_content):].strip()
                                        # Asegurar un separador limpio
                                        if section_content and not section_content.startswith("\n"):
                                            section_content = "\n\n" + section_content
                                    else:
                                        # Si no podemos identificar claramente la parte nueva,
                                        # usar solo los √∫ltimos p√°rrafos como contenido de secci√≥n
                                        paragraphs = recovered_content.split('\n\n')
                                        if len(paragraphs) > 3:
                                            section_content = '\n\n'.join(paragraphs[-3:])
                                        else:
                                            section_content = recovered_content
                                else:
                                    # Si no tuvimos √©xito, usar texto de fallback
                                    section_content = f"La narrativa continu√≥ desarroll√°ndose mientras los personajes exploraban los eventos relacionados con {idea[:30]}..."
                            else:
                                # Si no hay contenido previo, generar algo m√≠nimo
                                emergency_prompt = f"""
                                Escribe un p√°rrafo corto para abrir este cap√≠tulo.
                                T√≠tulo: "{chapter_title}"
                                Idea: {idea[:100]}
                                """
                                try:
                                    response = writer_chain.llm(emergency_prompt, temperature=0.7)
                                    section_content = clean_think_tags(extract_content_from_llm_response(response))
                                except:
                                    section_content = f"El cap√≠tulo comenz√≥ explorando {idea[:30]}..."
                        else:
                            # Otros tipos de errores, reenviar la excepci√≥n
                            raise
                    
                    # Procesar el contenido generado con el nuevo sistema
                    processed_content, analysis, is_collapsed = context_system.process_generated_content(
                        content=section_content,
                        section_position=section_position,
                        idea=idea
                    )
                    
                    # Si se detecta riesgo de colapso, regenerar la secci√≥n
                    if is_collapsed:
                        print_progress(f"‚ö†Ô∏è Regenerando secci√≥n con problemas: {analysis.get('analysis', {}).get('potential_issues', [])}")
                        
                        # Intentar regenerar la secci√≥n de forma segura
                        section_content = context_system.regenerate_section_safely(
                            llm=writer_chain.llm,
                            idea=idea,
                            section_position=section_position,
                            chapter_info=chapter_info
                        )
                    
                    # A√±adir contenido a las acumulaciones
                    chapter_content.append(section_content)
                    book[chapter].append(section_content)

                except Exception as e:
                    print_progress(f"Error generando contenido para idea {j}: {str(e)}")
                    print_progress("Intentando recuperar la generaci√≥n...")
                    
                    # Usar funci√≥n de recuperaci√≥n de emergencia
                    from utils import recover_from_model_collapse
                    
                    chapter_details = {
                        'title': chapter_title,
                        'summary': chapter_summary,
                        'idea': idea
                    }
                    
                    fallback_content = recover_from_model_collapse(
                        writer_chain.llm, 
                        chapter_details, 
                        context_system, 
                        section_position
                    )
                    
                    print_progress("Recuperaci√≥n con contenido alternativo")
                    chapter_content.append(fallback_content)
                    book[chapter].append(fallback_content)
                    continue
            
            # Al final de cada cap√≠tulo, generar resumen usando el nuevo sistema
            if len(chapter_content) > 0:
                # Generar resumen optimizado para mantener coherencia entre cap√≠tulos
                chapter_summary_final = context_system.create_chapter_summary(
                    llm=summary_chain.llm,
                    chapter_content=chapter_content,
                    chapter_info=chapter_info
                )
                
                # Guardar el resumen
                chapter_summaries[chapter] = chapter_summary_final
            
            print_progress(f"‚úì Cap√≠tulo {i} completado: {len(chapter_content)} secciones")
            
            # Resetear el contador de chequeos al cambiar de cap√≠tulo
            health_check_counter = 0

        print_progress("======================================")
        print_progress("ESCRITURA DEL LIBRO FINALIZADA")
        print_progress("======================================")
        return book

    except Exception as e:
        print_progress(f"Error general en la escritura del libro: {str(e)}")
        raise

def generate_chapter_content_for_limited_context(llm, chapter_details, context_manager, max_chunk_size=700):
    """
    Genera el contenido de un cap√≠tulo para modelos con contexto limitado como Gemma 9B
    utilizando una t√©cnica de generaci√≥n en bloques peque√±os con planificaci√≥n coherente.
    
    Args:
        llm: Modelo de lenguaje a utilizar
        chapter_details: Detalles del cap√≠tulo a generar
        context_manager: Gestor de contexto progresivo
        max_chunk_size: Tama√±o m√°ximo de cada bloque en palabras
        
    Returns:
        str: Contenido completo del cap√≠tulo
    """
    from time import sleep
    import re
    
    # 1. Crear un plan de alto nivel para el cap√≠tulo
    plan_prompt = f"""
    Crea un plan detallado para el cap√≠tulo '{chapter_details['title']}' de nuestro libro.
    
    Contexto del libro: {context_manager.get_minimal_context()}
    
    El plan debe incluir:
    1. 5-7 secciones clave con subt√≠tulos
    2. Para cada secci√≥n, 2-3 puntos principales a desarrollar
    3. Conexiones l√≥gicas entre secciones
    
    Formato simple:
    SECCI√ìN 1: [T√≠tulo]
    - Punto clave 1
    - Punto clave 2
    
    SECCI√ìN 2: [T√≠tulo]
    ...
    """
    
    print("Creando plan de cap√≠tulo...")
    chapter_plan = llm(plan_prompt, temperature=0.7)
    
    # 2. Extraer secciones del plan
    sections = []
    current_section = None
    
    for line in chapter_plan.split('\n'):
        line = line.strip()
        if not line:
            continue
            
        if line.startswith('SECCI√ìN') or re.match(r'^\d+\.', line):
            # Extraer t√≠tulo de secci√≥n
            if ':' in line:
                section_title = line.split(':', 1)[1].strip()
            else:
                section_title = line
                
            current_section = {'title': section_title, 'points': []}
            sections.append(current_section)
        elif line.startswith('-') and current_section:
            # A√±adir punto clave a la secci√≥n actual
            current_section['points'].append(line[1:].strip())
    
    # 3. Generar contenido secci√≥n por secci√≥n
    full_content = []
    previous_content = ""
    
    for i, section in enumerate(sections):
        print(f"Generando secci√≥n {i+1}/{len(sections)}: {section['title']}")
        
        # Contexto para continuidad entre secciones
        continuity_context = ""
        if i > 0:
            # Usar √∫ltimas frases de la secci√≥n anterior
            words = previous_content.split()
            if words:
                last_words = ' '.join(words[-min(50, len(words)):])
                continuity_context = f"La secci√≥n anterior termin√≥ as√≠: '{last_words}'"
        
        # Prompt para generar la secci√≥n
        section_prompt = f"""
        Escribe la secci√≥n '{section['title']}' del cap√≠tulo '{chapter_details['title']}' siguiendo estos puntos clave:
        
        {chr(10).join(['- ' + point for point in section['points']])}
        
        {continuity_context}
        
        Contexto del libro: {context_manager.get_minimal_context()}
        
        La secci√≥n debe:
        - Tener aproximadamente {max_chunk_size} palabras
        - Mantener un tono consistente con el resto del libro
        - Fluir naturalmente desde la secci√≥n anterior
        - No repetir informaci√≥n ya proporcionada
        """
        
        # Generar contenido de la secci√≥n
        section_content = llm(section_prompt, temperature=0.7)
        full_content.append(section_content)
        previous_content = section_content
        
        # Peque√±a pausa para evitar sobrecarga y permitir mejor planificaci√≥n
        sleep(1)
    
    # 4. Generar transiciones suaves entre secciones
    final_content = []
    
    for i, content in enumerate(full_content):
        final_content.append(content)
        
        # Generar transici√≥n si no es la √∫ltima secci√≥n
        if i < len(full_content) - 1:
            # Extraer √∫ltima frase de la secci√≥n actual
            current_last_sentence = extract_last_sentence(content)
            
            # Extraer primera frase de la siguiente secci√≥n
            next_first_sentence = extract_first_sentence(full_content[i+1])
            
            # Generar transici√≥n coherente
            transition_prompt = f"""
            Crea una transici√≥n suave de 1-2 frases entre estas ideas:
            
            Idea actual: "{current_last_sentence}"
            
            Siguiente idea: "{next_first_sentence}"
            """
            
            transition = llm(transition_prompt, temperature=0.7)
            final_content.append(transition)
    
    # 5. Unir todo el contenido
    complete_chapter = '\n\n'.join(final_content)
    
    # 6. Revisi√≥n final de coherencia (opcional para modelos muy limitados)
    if context_manager.model_capacity != "limited":
        coherence_prompt = f"""
        Revisa este contenido para asegurar coherencia y fluidez. Corrige solo problemas graves de
        continuidad o contradicciones. No cambies el estilo ni la sustancia.
        
        {complete_chapter[:1000]}
        ...
        {complete_chapter[-1000:]}
        """
        
        complete_chapter = llm(coherence_prompt, temperature=0.2)
    
    return complete_chapter

def extract_first_sentence(text):
    """Extrae la primera frase de un texto"""
    sentences = re.split(r'(?<=[.!?])\s+', text.strip())
    return sentences[0] if sentences else ""

def extract_last_sentence(text):
    """Extrae la √∫ltima frase de un texto"""
    sentences = re.split(r'(?<=[.!?])\s+', text.strip())
    return sentences[-1] if sentences else ""

def optimize_prompt_for_limited_context(prompt, max_length=800, preserve_instructions=True):
    """
    Optimiza un prompt para modelos con contexto limitado como Gemma 9B
    
    Args:
        prompt: El prompt original
        max_length: Longitud m√°xima deseada en caracteres
        preserve_instructions: Si se deben preservar las instrucciones completas
        
    Returns:
        str: Prompt optimizado
    """
    if len(prompt) <= max_length:
        return prompt
        
    # Dividir en componentes
    parts = prompt.split('\n\n')
    
    # Identificar instrucciones (generalmente al inicio del prompt)
    instructions = parts[0] if parts else ""
    content_parts = parts[1:] if parts else []
    
    # Preservar instrucciones completas si es necesario
    if preserve_instructions:
        optimized_parts = [instructions]
    else:
        # Resumir instrucciones si son muy largas
        if len(instructions) > max_length // 3:
            # Tomar solo primeras l√≠neas de instrucciones
            instruction_lines = instructions.split('\n')
            optimized_parts = ['\n'.join(instruction_lines[:3])]
        else:
            optimized_parts = [instructions]
    
    # Calcular espacio disponible para contenido
    used_length = sum(len(part) for part in optimized_parts)
    available_length = max_length - used_length
    
    # Estrategia: preservar partes m√°s informativas
    # Normalmente las primeras y √∫ltimas partes contienen informaci√≥n cr√≠tica
    if content_parts and available_length > 0:
        # Siempre incluir primera parte (contexto/tarea principal)
        first_part = content_parts[0]
        if len(first_part) <= available_length * 0.5:
            optimized_parts.append(first_part)
            available_length -= len(first_part)
            content_parts.pop(0)
        else:
            # Truncar primera parte si es necesario
            optimized_parts.append(first_part[:int(available_length * 0.5)])
            available_length -= int(available_length * 0.5)
        
        # Intentar incluir √∫ltima parte (suele contener el formato deseado)
        if content_parts and available_length > 0:
            last_part = content_parts[-1]
            if len(last_part) <= available_length:
                optimized_parts.append(last_part)
                available_length -= len(last_part)
                content_parts.pop()
            else:
                # Truncar √∫ltima parte si es necesario
                optimized_parts.append(last_part[-int(available_length):])
                available_length = 0
    
    # Unir partes optimizadas
    optimized_prompt = '\n\n'.join(optimized_parts)
    
    return optimized_prompt
